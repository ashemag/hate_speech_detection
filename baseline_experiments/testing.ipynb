{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "from globals import ROOT_DIR\n",
    "from data_providers import TextDataProvider\n",
    "import argparse\n",
    "import configparser\n",
    "from torch import optim\n",
    "from experiment_builder import ExperimentBuilder\n",
    "from data_providers import *\n",
    "import os\n",
    "from models.cnn import *\n",
    "from models.multilayer_perceptron import multi_layer_perceptron\n",
    "import tweepy\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../config.ini')\n",
    "path_data = os.path.join(ROOT_DIR, config['DEFAULT']['PATH_DATA'])\n",
    "path_labels = os.path.join(ROOT_DIR, config['DEFAULT']['PATH_LABELS'])\n",
    "\n",
    "consumer_key = config['DEFAULT']['TWITTER_CONSUMER_KEY']\n",
    "consumer_secret_key = config['DEFAULT']['TWITTER_CONSUMER_SECRET_KEY']\n",
    "access_token = config['DEFAULT']['TWITTER_ACCESS_TOKEN']\n",
    "access_token_secret = config['DEFAULT']['TWITTER_ACCESS_TOKEN_SECRET']\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret_key)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64149"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load(os.path.join(ROOT_DIR, 'data/founta_data.npy'))\n",
    "data = data[()]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19146050600944675"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_ids = [value['in_reply_to_status_id'] for key, value in data.items() if value['in_reply_to_status_id'] is not None]\n",
    "len(status_ids) / len(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start ptr is at 0\n",
      "82\n",
      "Start ptr is at 100\n",
      "80\n",
      "Start ptr is at 200\n",
      "75\n",
      "Start ptr is at 300\n",
      "85\n",
      "Start ptr is at 400\n",
      "76\n",
      "Start ptr is at 500\n",
      "83\n",
      "Start ptr is at 600\n",
      "72\n",
      "Start ptr is at 700\n",
      "81\n",
      "Start ptr is at 800\n",
      "85\n",
      "Start ptr is at 900\n",
      "87\n",
      "Start ptr is at 1000\n",
      "85\n",
      "Start ptr is at 1100\n",
      "80\n",
      "Start ptr is at 1200\n",
      "89\n",
      "Start ptr is at 1300\n",
      "89\n",
      "Start ptr is at 1400\n",
      "91\n",
      "Start ptr is at 1500\n",
      "85\n",
      "Start ptr is at 1600\n",
      "86\n",
      "Start ptr is at 1700\n",
      "87\n",
      "Start ptr is at 1800\n",
      "81\n",
      "Start ptr is at 1900\n",
      "86\n",
      "Start ptr is at 2000\n",
      "87\n",
      "Start ptr is at 2100\n",
      "85\n",
      "Start ptr is at 2200\n",
      "74\n",
      "Start ptr is at 2300\n",
      "88\n",
      "Start ptr is at 2400\n",
      "91\n",
      "Start ptr is at 2500\n",
      "85\n",
      "Start ptr is at 2600\n",
      "82\n",
      "Start ptr is at 2700\n",
      "93\n",
      "Start ptr is at 2800\n",
      "80\n",
      "Start ptr is at 2900\n",
      "82\n",
      "Start ptr is at 3000\n",
      "82\n",
      "Start ptr is at 3100\n",
      "79\n",
      "Start ptr is at 3200\n",
      "88\n",
      "Start ptr is at 3300\n",
      "85\n",
      "Start ptr is at 3400\n",
      "88\n",
      "Start ptr is at 3500\n",
      "86\n",
      "Start ptr is at 3600\n",
      "90\n",
      "Start ptr is at 3700\n",
      "86\n",
      "Start ptr is at 3800\n",
      "83\n",
      "Start ptr is at 3900\n",
      "89\n",
      "Start ptr is at 4000\n",
      "84\n",
      "Start ptr is at 4100\n",
      "78\n",
      "Start ptr is at 4200\n",
      "86\n",
      "Start ptr is at 4300\n",
      "86\n",
      "Start ptr is at 4400\n",
      "84\n",
      "Start ptr is at 4500\n",
      "89\n",
      "Start ptr is at 4600\n",
      "85\n",
      "Start ptr is at 4700\n",
      "83\n",
      "Start ptr is at 4800\n",
      "86\n",
      "Start ptr is at 4900\n",
      "84\n",
      "Start ptr is at 5000\n",
      "91\n",
      "Start ptr is at 5100\n",
      "89\n",
      "Start ptr is at 5200\n",
      "80\n",
      "Start ptr is at 5300\n",
      "95\n",
      "Start ptr is at 5400\n",
      "89\n",
      "Start ptr is at 5500\n",
      "85\n",
      "Start ptr is at 5600\n",
      "90\n",
      "Start ptr is at 5700\n",
      "87\n",
      "Start ptr is at 5800\n",
      "87\n",
      "Start ptr is at 5900\n",
      "85\n",
      "Start ptr is at 6000\n",
      "87\n",
      "Start ptr is at 6100\n",
      "91\n",
      "Start ptr is at 6200\n",
      "84\n",
      "Start ptr is at 6300\n",
      "87\n",
      "Start ptr is at 6400\n",
      "87\n",
      "Start ptr is at 6500\n",
      "88\n",
      "Start ptr is at 6600\n",
      "84\n",
      "Start ptr is at 6700\n",
      "84\n",
      "Start ptr is at 6800\n",
      "89\n",
      "Start ptr is at 6900\n",
      "91\n",
      "Start ptr is at 7000\n",
      "85\n",
      "Start ptr is at 7100\n",
      "84\n",
      "Start ptr is at 7200\n",
      "89\n",
      "Start ptr is at 7300\n",
      "85\n",
      "Start ptr is at 7400\n",
      "83\n",
      "Start ptr is at 7500\n",
      "89\n",
      "Start ptr is at 7600\n",
      "88\n",
      "Start ptr is at 7700\n",
      "87\n",
      "Start ptr is at 7800\n",
      "83\n",
      "Start ptr is at 7900\n",
      "85\n",
      "Start ptr is at 8000\n",
      "88\n",
      "Start ptr is at 8100\n",
      "92\n",
      "Start ptr is at 8200\n",
      "87\n",
      "Start ptr is at 8300\n",
      "88\n",
      "Start ptr is at 8400\n",
      "90\n",
      "Start ptr is at 8500\n",
      "87\n",
      "Start ptr is at 8600\n",
      "85\n",
      "Start ptr is at 8700\n",
      "84\n",
      "Start ptr is at 8800\n",
      "91\n",
      "Start ptr is at 8900\n",
      "89\n",
      "Start ptr is at 9000\n",
      "89\n",
      "Start ptr is at 9100\n",
      "84\n",
      "Start ptr is at 9200\n",
      "86\n",
      "Start ptr is at 9300\n",
      "86\n",
      "Start ptr is at 9400\n",
      "90\n",
      "Start ptr is at 9500\n",
      "86\n",
      "Start ptr is at 9600\n",
      "81\n",
      "Start ptr is at 9700\n",
      "90\n",
      "Start ptr is at 9800\n",
      "82\n",
      "Start ptr is at 9900\n",
      "91\n",
      "Start ptr is at 10000\n",
      "82\n",
      "Start ptr is at 10100\n",
      "87\n",
      "Start ptr is at 10200\n",
      "84\n",
      "Start ptr is at 10300\n",
      "89\n",
      "Start ptr is at 10400\n",
      "80\n",
      "Start ptr is at 10500\n",
      "93\n",
      "Start ptr is at 10600\n",
      "89\n",
      "Start ptr is at 10700\n",
      "80\n",
      "Start ptr is at 10800\n",
      "89\n",
      "Start ptr is at 10900\n",
      "88\n",
      "Start ptr is at 11000\n",
      "92\n",
      "Start ptr is at 11100\n",
      "86\n",
      "Start ptr is at 11200\n",
      "76\n",
      "Start ptr is at 11300\n",
      "77\n",
      "Start ptr is at 11400\n",
      "86\n",
      "Start ptr is at 11500\n",
      "80\n",
      "Start ptr is at 11600\n",
      "78\n",
      "Start ptr is at 11700\n",
      "82\n",
      "Start ptr is at 11800\n",
      "83\n",
      "Start ptr is at 11900\n",
      "80\n",
      "Start ptr is at 12000\n",
      "78\n",
      "Start ptr is at 12100\n",
      "86\n",
      "Start ptr is at 12200\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "#collected tweets from tweepy \n",
    "start_ptr = 0\n",
    "end_ptr = start_ptr + 100 \n",
    "replies = {}\n",
    "while(start_ptr <= len(status_ids)):\n",
    "    print(\"Start ptr is at {}\".format(start_ptr))\n",
    "    reply_tweets = api.statuses_lookup(status_ids[start_ptr:end_ptr],trim_user=True)\n",
    "    print(len(reply_tweets))\n",
    "    \n",
    "    # load and save results each time \n",
    "    if start_ptr != 0:\n",
    "        replies = np.load(os.path.join(ROOT_DIR, 'data/reply_data.npy'))\n",
    "        replies = replies[()]\n",
    "    for i, reply_tweet in enumerate(reply_tweets):\n",
    "        reply_tweet = reply_tweet._json\n",
    "        replies[reply_tweet['id_str']] = reply_tweet['text']\n",
    "    np.save(os.path.join(ROOT_DIR, 'data/reply_data.npy'), replies)\n",
    "\n",
    "    start_ptr += 100 \n",
    "    end_ptr += 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10283"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies = np.load(os.path.join(ROOT_DIR, 'data/reply_data.npy'))\n",
    "replies = replies[()]\n",
    "len(replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14598599576616186\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count = 0 \n",
    "# 0.16608010725657785 deleted... \n",
    "for status_id in status_ids:\n",
    "    if str(status_id) not in replies:\n",
    "        count += 1\n",
    "print(count / len(status_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'         '"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verifies missing tweet \n",
    "# missing_tweet = api.statuses_lookup([847652506372984835],trim_user=True)\n",
    "# missing_tweet\n",
    "hey = '         '\n",
    "hey.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining tweets \n",
    "status_ids_fetched = []\n",
    "outputs_context = []\n",
    "for output in outputs:\n",
    "    status_id = str(output['in_reply_to_status_id'])\n",
    "    if status_id in replies:\n",
    "        output['reply_to_tweet_text'] = output['text'] + replies[status_id]\n",
    "    else:\n",
    "        output['reply_to_tweet_text'] = output['text'] + output['text']\n",
    "    outputs_context.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "\n",
    "def tokenize(outpus):\n",
    "    key = 'reply_to_tweet_text'\n",
    "    outputs_processed = []\n",
    "    for output in outputs:\n",
    "        text = output[key]\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        tokens = text.split(' ')\n",
    "        output['tokens'] = tokens\n",
    "        outputs_processed.append(output)\n",
    "    return outputs_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_processed = tokenize(outputs_context)\n",
    "outputs_processed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid, x_test, y_test = split_data(outputs_processed, labels, 28)\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_EMBED_DIM = 300\n",
    "TWITTER_EMBED_DIM = 400\n",
    "TWEET_SENTENCE_SIZE = 17*2 # 16 is average tweet token length\n",
    "TWEET_WORD_SIZE = 20 # selected by histogram of tweet counts\n",
    "FASTTEXT_EMBED_DIM = 300\n",
    "EMBED_DIM = 200\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "def generate_random_embedding(embed_dim):\n",
    "    return np.random.normal(scale=0.6, size=(embed_dim,))\n",
    "\n",
    "\n",
    "# embeds tokens! \n",
    "def fetch_word_embeddings(outputs, word_vectors, embed_dim):\n",
    "    outputs_embed = [] \n",
    "    for i, output in enumerate(outputs):\n",
    "        tweet = output['tokens']\n",
    "        embedded_tweet = []\n",
    "\n",
    "        # trim if too large\n",
    "        if len(tweet) >= TWEET_SENTENCE_SIZE:\n",
    "            tweet = tweet[:TWEET_SENTENCE_SIZE]\n",
    "\n",
    "        # convert all into word embeddings\n",
    "        for word in tweet:\n",
    "            embedding = generate_random_embedding(embed_dim) if word not in word_vectors else word_vectors[word]\n",
    "            embedded_tweet.append(embedding)\n",
    "\n",
    "        # pad if too short\n",
    "        if len(tweet) < TWEET_SENTENCE_SIZE:\n",
    "            diff = TWEET_SENTENCE_SIZE - len(tweet)\n",
    "            embedded_tweet += [generate_random_embedding(embed_dim) for _ in range(diff)]\n",
    "\n",
    "        assert len(embedded_tweet) == TWEET_SENTENCE_SIZE\n",
    "        output['word_embeddings'] = embedded_tweet\n",
    "        outputs_embed.append(output)\n",
    "    return outputs_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "embed_dim = 400\n",
    "filename = os.path.join(ROOT_DIR, 'data/word2vec_twitter_model/word2vec_twitter_model.bin')\n",
    "word_vectors = KeyedVectors.load_word2vec_format(filename, binary=True, unicode_errors='ignore')\n",
    "print(\"Total time {} min\".format((time.time() - start) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "x_train_embed = fetch_word_embeddings(x_train, word_vectors, embed_dim)\n",
    "x_valid_embed = fetch_word_embeddings(x_valid, word_vectors, embed_dim)\n",
    "x_test_embed = fetch_word_embeddings(x_test, word_vectors, embed_dim)\n",
    "print(\"Total time {} min\".format((time.time() - start) / 60))\n",
    "print(len(x_train_embed), len(x_valid_embed), len(x_test_embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_embed[0].keys() # list of dictionaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0] #int "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17 n-gram \n",
    "# retweet count \n",
    "# in reply to status id \n",
    "# favorite count \n",
    "\n",
    "# 34 n-gram \n",
    "def convert_to_feature_embeddings(x_embed):\n",
    "    return [x['word_embeddings'] for x in x_embed]\n",
    " \n",
    "data = {}\n",
    "print(x_train_embed[0].keys())\n",
    "data['x_train'] = convert_to_feature_embeddings(x_train_embed)\n",
    "data['y_train'] = y_train\n",
    "data['x_valid'] = convert_to_feature_embeddings(x_valid_embed)\n",
    "data['y_valid'] = y_valid\n",
    "data['x_test'] = convert_to_feature_embeddings(x_test_embed)\n",
    "data['y_test'] = y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "def wrap_data(batch_size, seed, x_train, y_train, x_valid, y_valid, x_test, y_test):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    transform=None\n",
    "    \n",
    "    train_set = DataProvider(inputs=x_train, targets=y_train, seed=seed, transform=transform)\n",
    "    train_data_local = torch.utils.data.DataLoader(train_set,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   num_workers=2,\n",
    "                                                   sampler=ImbalancedDatasetSampler(train_set),\n",
    "                                                   )\n",
    "\n",
    "    valid_set = DataProvider(inputs=x_valid, targets=y_valid, seed=seed, transform=transform)\n",
    "    valid_data_local = torch.utils.data.DataLoader(valid_set,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   num_workers=2,\n",
    "                                                   shuffle=False,\n",
    "                                                  )\n",
    "\n",
    "    test_set = DataProvider(inputs=x_test, targets=y_test, seed=seed, transform=transform)\n",
    "    test_data_local = torch.utils.data.DataLoader(test_set,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  num_workers=2,\n",
    "                                                  shuffle=False,\n",
    "                                                 )\n",
    "    return train_data_local, valid_data_local, test_data_local\n",
    "\n",
    "def fetch_model(model, embedding_level, input_shape_local, dropout):\n",
    "    if model == 'MLP':\n",
    "        return multi_layer_perceptron(input_shape_local)\n",
    "    if model == 'CNN':\n",
    "        if embedding_level == 'word':\n",
    "            return word_cnn(input_shape_local, dropout)\n",
    "        elif embedding_level == 'character':\n",
    "            return character_cnn(input_shape_local)\n",
    "    if model == 'DENSENET':\n",
    "        return densenet()\n",
    "    else:\n",
    "        raise ValueError(\"Model key not found {}\".format(embedding_level))\n",
    "\n",
    "\n",
    "def fetch_model_parameters(input_shape_local):\n",
    "    model_local = fetch_model(model='CNN',\n",
    "                            embedding_level='word',\n",
    "                            input_shape_local=input_shape_local,\n",
    "                            dropout=0.5)\n",
    "    criterion_local = torch.nn.CrossEntropyLoss()\n",
    "    optimizer_local = torch.optim.Adam(model_local.parameters(), weight_decay=1e-4)\n",
    "    scheduler_local = optim.lr_scheduler.CosineAnnealingLR(optimizer_local, T_max=100, eta_min=0.0001)\n",
    "    return model_local, criterion_local, optimizer_local, scheduler_local\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = wrap_data(2048, 28, **data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in train_data:\n",
    "    input_shape = x.shape\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = tuple(input_shape)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train_iter(model, device, optimizer, criterion, x, y, stats, experiment_key='train'):\n",
    "    \"\"\"\n",
    "    Receives the inputs and targets for the model and runs a training iteration. Returns loss and accuracy metrics.\n",
    "    :param x: The inputs to the model. A numpy array of shape batch_size, channels, height, width\n",
    "    :param y: The targets for the model. A numpy array of shape batch_size, num_classes\n",
    "    :return: the loss and accuracy for this batch\n",
    "    \"\"\"\n",
    "    # sets model to training mode\n",
    "    # (in case batch normalization or other methods have different procedures for training and evaluation)\n",
    "    model.train()\n",
    "    x = x.float()\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    optimizer.zero_grad()  # set all weight grads from previous training iters to 0\n",
    "    out = model.forward(x)  # forward the data in the model\n",
    "    # loss = F.cross_entropy(input=out, target=y)  # compute loss\n",
    "    loss = criterion(out, y)\n",
    "    loss.backward()  # backpropagate to compute gradients for current iter loss\n",
    "\n",
    "    optimizer.step()  # update network parameters\n",
    "    _, predicted = torch.max(out.data, 1)  # get argmax of predictions\n",
    "    accuracy = np.mean(list(predicted.eq(y.data).cpu()))  # compute accuracy\n",
    "    stats['{}_acc'.format(experiment_key)].append(accuracy)\n",
    "    stats['{}_loss'.format(experiment_key)].append(loss.data.detach().cpu().numpy())\n",
    "\n",
    "def run_evaluation_iter(model, device, optimizer, criterion, x, y, stats, experiment_key='valid'):\n",
    "    \"\"\"\n",
    "    Receives the inputs and targets for the model and runs an evaluation iterations. Returns loss and accuracy metrics.\n",
    "    :param x: The inputs to the model. A numpy array of shape batch_size, channels, height, width\n",
    "    :param y: The targets for the model. A numpy array of shape batch_size, num_classes\n",
    "    :return: the loss and accuracy for this batch\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        model.eval()  # sets the system to validation mode\n",
    "        x = x.float()\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        out = model.forward(x)  # forward the data in the model\n",
    "        loss = criterion(out, y)\n",
    "        \n",
    "        # loss = F.cross_entropy(out, y)  # compute loss\n",
    "        _, predicted = torch.max(out.data, 1)  # get argmax of predictions\n",
    "        \n",
    "        accuracy = np.mean(list(predicted.eq(y.data).cpu()))\n",
    "        stats['{}_acc'.format(experiment_key)].append(accuracy)  # compute accuracy\n",
    "        stats['{}_loss'.format(experiment_key)].append(loss.data.detach().cpu().numpy())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_save_dir, model_save_name, model_idx):\n",
    "    \"\"\"\n",
    "    Save the network parameter state and current best val epoch idx and best val accuracy.\n",
    "    :param model_save_name: Name to use to save model without the epoch index\n",
    "    :param model_idx: The index to save the model with.\n",
    "    :param best_validation_model_idx: The index of the best validation model to be stored for future use.\n",
    "    :param best_validation_model_acc: The best validation accuracy to be stored for use at test time.\n",
    "    :param model_save_dir: The directory to store the state at.\n",
    "    :param state: The dictionary containing the system state.\n",
    "\n",
    "    \"\"\"\n",
    "    # Save state each epoch\n",
    "    path = os.path.join(model_save_dir, \"{}_{}\".format(model_save_name, str(model_idx)))\n",
    "    torch.save(model.state_dict(), f=path)\n",
    "    \n",
    "\n",
    "def load_model(model, model_save_dir, model_save_name, model_idx):\n",
    "    \"\"\"\n",
    "    Load the network parameter state and the best val model idx and best val acc to be compared with the future val accuracies, in order to choose the best val model\n",
    "    :param model_save_dir: The directory to store the state at.\n",
    "    :param model_save_name: Name to use to save model without the epoch index\n",
    "    :param model_idx: The index to save the model with.\n",
    "    \"\"\"\n",
    "    path = os.path.join(model_save_dir, \"{}_{}\".format(model_save_name, str(model_idx)))\n",
    "    checkpoint = torch.load(f=path)\n",
    "    # freeze parameters\n",
    "    model.load_state_dict(checkpoint)\n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict, defaultdict\n",
    "import tqdm\n",
    "\n",
    "model, criterion, optimizer, _ = fetch_model_parameters(input_shape)\n",
    "device = torch.device('cpu')\n",
    "train_stats = OrderedDict()\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    epoch_stats = defaultdict(list)\n",
    "    with tqdm.tqdm(total=len(train_data)) as pbar_train:  # create a progress bar for training\n",
    "        for idx, (x, y) in enumerate(train_data):  # get data batches\n",
    "            run_train_iter(model, device, optimizer, criterion, x=x, y=y, stats=epoch_stats)  # take a training iter step\n",
    "            pbar_train.update(1)\n",
    "            pbar_train.set_description(\"loss: {:.4f}, accuracy: {:.4f}\".format(epoch_stats['train_loss'][-1],\n",
    "                                                                               epoch_stats['train_acc'][-1]))\n",
    "\n",
    "    with tqdm.tqdm(total=len(valid_data)) as pbar_val:  # create a progress bar for validation\n",
    "        for x, y in valid_data:  # get data batches\n",
    "            run_evaluation_iter(model, device, optimizer, criterion, x=x, y=y, stats=epoch_stats)  # run a validation iter\n",
    "            pbar_val.update(1)  # add 1 step to the progress bar\n",
    "            pbar_val.set_description(\"loss: {:.4f}, accuracy: {:.4f}\".format(epoch_stats['valid_loss'][-1],\n",
    "                                                                             epoch_stats['valid_acc'][-1]))\n",
    "     \n",
    "    \n",
    "    \n",
    "    save_model(model, '', 'testing', epoch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model(model, '', 'testing', 1)\n",
    "\n",
    "#evaluate test here\n",
    "with tqdm.tqdm(total=len(test_data)) as pbar_test:  # create a progress bar for validation\n",
    "    for x, y in test_data:  # get data batches\n",
    "        run_evaluation_iter(model, device, optimizer, criterion, x=x, y=y, stats=epoch_stats, experiment_key=\"test_experiment\")  # run a validation iter\n",
    "        pbar_test.update(1)  # add 1 step to the progress bar\n",
    "        pbar_test.set_description(\"loss: {:.4f}, accuracy: {:.4f}\".format(epoch_stats['test_experiment_loss'][-1],\n",
    "                                                                  epoch_stats['test_experiment_acc'][-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "for i in range(17):\n",
    "    arr.append(np.zeros(200,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 200)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(arr).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_tweet = np.array(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_tweet += np.array(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 200)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_tweet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_tweet = np.array(arr) + np.array(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_tweet = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(17):\n",
    "    blank_embedding = np.zeros(200,)\n",
    "    embedded_tweet.append(blank_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 200)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(embedded_tweet).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
