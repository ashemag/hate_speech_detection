{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "from globals import ROOT_DIR\n",
    "from data_providers import TextDataProvider\n",
    "import argparse\n",
    "import configparser\n",
    "from torch import optim\n",
    "from experiment_builder import ExperimentBuilder\n",
    "from data_providers import *\n",
    "import os\n",
    "from models.cnn import *\n",
    "from models.multilayer_perceptron import multi_layer_perceptron\n",
    "import tweepy\n",
    "from utils import *\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../config.ini')\n",
    "path_data = os.path.join(ROOT_DIR, config['DEFAULT']['PATH_DATA'])\n",
    "path_labels = os.path.join(ROOT_DIR, config['DEFAULT']['PATH_LABELS'])\n",
    "\n",
    "consumer_key = config['DEFAULT']['TWITTER_CONSUMER_KEY']\n",
    "consumer_secret_key = config['DEFAULT']['TWITTER_CONSUMER_SECRET_KEY']\n",
    "access_token = config['DEFAULT']['TWITTER_ACCESS_TOKEN']\n",
    "access_token_secret = config['DEFAULT']['TWITTER_ACCESS_TOKEN_SECRET']\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret_key)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "data = np.load(os.path.join(ROOT_DIR, 'data/founta_data.npy'))\n",
    "data = data[()]\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data = pd.read_csv(os.path.join(ROOT_DIR, 'data/founta_data.csv'), header='infer', index_col=0, squeeze=True).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99799"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Extracting tweets from JSON ===\n"
     ]
    }
   ],
   "source": [
    "outputs, labels = extract_tweets(label_data, data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Extracting tweets from JSON ===\n"
     ]
    }
   ],
   "source": [
    "data_provider = TextDataProvider(os.path.join(ROOT_DIR, 'data/founta_data.npy'), os.path.join(ROOT_DIR, 'data/founta_data.csv'), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Bert, Processed 1 / 11\n",
      "Downloading Bert, Processed 2 / 11\n",
      "Downloading Bert, Processed 3 / 11\n",
      "Downloading Bert, Processed 4 / 11\n",
      "Downloading Bert, Processed 5 / 11\n",
      "Downloading Bert, Processed 6 / 11\n",
      "Downloading Bert, Processed 7 / 11\n",
      "Downloading Bert, Processed 8 / 11\n",
      "Downloading Bert, Processed 9 / 11\n",
      "Downloading Bert, Processed 10 / 11\n",
      "Downloading Bert, Processed 11 / 11\n"
     ]
    }
   ],
   "source": [
    "embeddings = data_provider.generate_bert_embedding_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10283"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply_data = np.load(os.path.join(ROOT_DIR, 'data/reply_data.npy'))\n",
    "reply_data = reply_data[()]\n",
    "\n",
    "len(reply_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-661781cc409e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mfinal_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_embed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtweet_embed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mfinal_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_embed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreply_status_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mdata_embed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_embed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "happens = 0\n",
    "data_embed = []\n",
    "no_longer_available = 0\n",
    "for output in outputs:\n",
    "    tweet_embed = embeddings[int(output['id'])]\n",
    "    reply_status_id = int(output['in_reply_to_status_id']) \n",
    "    final_embed = []\n",
    "    if reply_status_id != -1 and reply_status_id not in embeddings:\n",
    "        no_longer_available += 1\n",
    "    if reply_status_id == -1 or reply_status_id not in embeddings:\n",
    "        for i in range(17):\n",
    "            blank_embedding = np.zeros(768, )\n",
    "            final_embed.append(blank_embedding)\n",
    "        final_embed = final_embed + tweet_embed\n",
    "    else: \n",
    "        final_embed = final_embed + embeddings[reply_status_id]\n",
    "    data_embed.append(final_embed)\n",
    "    return data_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_embed[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1638169679205341"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_ids = [value['in_reply_to_status_id'] for key, value in data.items() if value['in_reply_to_status_id'] is not None]\n",
    "(len(status_ids) - len(replies)) / len(status_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64149\n",
      "Start ptr is at 0\n",
      "Start ptr is at 100\n",
      "Start ptr is at 200\n",
      "Start ptr is at 300\n",
      "Start ptr is at 400\n",
      "Start ptr is at 500\n",
      "Start ptr is at 600\n",
      "Start ptr is at 700\n",
      "Start ptr is at 800\n",
      "Start ptr is at 900\n",
      "Start ptr is at 1000\n",
      "Start ptr is at 1100\n",
      "Start ptr is at 1200\n",
      "Start ptr is at 1300\n",
      "Start ptr is at 1400\n",
      "Start ptr is at 1500\n",
      "Start ptr is at 1600\n",
      "Start ptr is at 1700\n",
      "Start ptr is at 1800\n",
      "Start ptr is at 1900\n",
      "Start ptr is at 2000\n",
      "Start ptr is at 2100\n",
      "Start ptr is at 2200\n",
      "Start ptr is at 2300\n",
      "Start ptr is at 2400\n",
      "Start ptr is at 2500\n",
      "Start ptr is at 2600\n",
      "Start ptr is at 2700\n",
      "Start ptr is at 2800\n",
      "Start ptr is at 2900\n",
      "Start ptr is at 3000\n",
      "Start ptr is at 3100\n",
      "Start ptr is at 3200\n",
      "Start ptr is at 3300\n",
      "Start ptr is at 3400\n",
      "Start ptr is at 3500\n",
      "Start ptr is at 3600\n",
      "Start ptr is at 3700\n",
      "Start ptr is at 3800\n",
      "Start ptr is at 3900\n",
      "Start ptr is at 4000\n",
      "Start ptr is at 4100\n",
      "Start ptr is at 4200\n",
      "Start ptr is at 4300\n",
      "Start ptr is at 4400\n",
      "Start ptr is at 4500\n",
      "Start ptr is at 4600\n",
      "Start ptr is at 4700\n",
      "Start ptr is at 4800\n",
      "Start ptr is at 4900\n",
      "Start ptr is at 5000\n",
      "Start ptr is at 5100\n",
      "Start ptr is at 5200\n",
      "Start ptr is at 5300\n",
      "Start ptr is at 5400\n",
      "Start ptr is at 5500\n",
      "Start ptr is at 5600\n",
      "Start ptr is at 5700\n",
      "Start ptr is at 5800\n",
      "Start ptr is at 5900\n",
      "Start ptr is at 6000\n",
      "Start ptr is at 6100\n",
      "Start ptr is at 6200\n",
      "Start ptr is at 6300\n",
      "Start ptr is at 6400\n",
      "Start ptr is at 6500\n",
      "Start ptr is at 6600\n",
      "Start ptr is at 6700\n",
      "Start ptr is at 6800\n",
      "Start ptr is at 6900\n",
      "Start ptr is at 7000\n",
      "Start ptr is at 7100\n",
      "Start ptr is at 7200\n",
      "Start ptr is at 7300\n",
      "Start ptr is at 7400\n",
      "Start ptr is at 7500\n",
      "Start ptr is at 7600\n",
      "Start ptr is at 7700\n",
      "Start ptr is at 7800\n",
      "Start ptr is at 7900\n",
      "Start ptr is at 8000\n",
      "Start ptr is at 8100\n",
      "Start ptr is at 8200\n",
      "Start ptr is at 8300\n",
      "Start ptr is at 8400\n",
      "Start ptr is at 8500\n",
      "Start ptr is at 8600\n",
      "Start ptr is at 8700\n",
      "Start ptr is at 8800\n",
      "Start ptr is at 8900\n",
      "Start ptr is at 9000\n",
      "Start ptr is at 9100\n",
      "Start ptr is at 9200\n",
      "Start ptr is at 9300\n",
      "Start ptr is at 9400\n",
      "Start ptr is at 9500\n",
      "Start ptr is at 9600\n",
      "Start ptr is at 9700\n",
      "Start ptr is at 9800\n",
      "Start ptr is at 9900\n",
      "Start ptr is at 10000\n",
      "Start ptr is at 10100\n",
      "Start ptr is at 10200\n",
      "Start ptr is at 10300\n",
      "Start ptr is at 10400\n",
      "Start ptr is at 10500\n",
      "Start ptr is at 10600\n",
      "Start ptr is at 10700\n",
      "Start ptr is at 10800\n",
      "Start ptr is at 10900\n",
      "Start ptr is at 11000\n",
      "Start ptr is at 11100\n",
      "Start ptr is at 11200\n",
      "Start ptr is at 11300\n",
      "Start ptr is at 11400\n",
      "Start ptr is at 11500\n",
      "Start ptr is at 11600\n",
      "Start ptr is at 11700\n",
      "Start ptr is at 11800\n",
      "Start ptr is at 11900\n",
      "Start ptr is at 12000\n",
      "Start ptr is at 12100\n",
      "Start ptr is at 12200\n",
      "Start ptr is at 12300\n",
      "Start ptr is at 12400\n",
      "Start ptr is at 12500\n",
      "Start ptr is at 12600\n",
      "Start ptr is at 12700\n",
      "Start ptr is at 12800\n",
      "Start ptr is at 12900\n",
      "Start ptr is at 13000\n",
      "Start ptr is at 13100\n",
      "Start ptr is at 13200\n",
      "Start ptr is at 13300\n",
      "Start ptr is at 13400\n",
      "Start ptr is at 13500\n",
      "Start ptr is at 13600\n",
      "Start ptr is at 13700\n",
      "Start ptr is at 13800\n",
      "Start ptr is at 13900\n",
      "Start ptr is at 14000\n",
      "Start ptr is at 14100\n",
      "Start ptr is at 14200\n",
      "Start ptr is at 14300\n",
      "Start ptr is at 14400\n",
      "Start ptr is at 14500\n",
      "Start ptr is at 14600\n",
      "Start ptr is at 14700\n",
      "Start ptr is at 14800\n",
      "Start ptr is at 14900\n",
      "Start ptr is at 15000\n",
      "Start ptr is at 15100\n",
      "Start ptr is at 15200\n",
      "Start ptr is at 15300\n",
      "Start ptr is at 15400\n",
      "Start ptr is at 15500\n",
      "Start ptr is at 15600\n",
      "Start ptr is at 15700\n",
      "Start ptr is at 15800\n",
      "Start ptr is at 15900\n",
      "Start ptr is at 16000\n",
      "Start ptr is at 16100\n",
      "Start ptr is at 16200\n",
      "Start ptr is at 16300\n",
      "Start ptr is at 16400\n",
      "Start ptr is at 16500\n",
      "Start ptr is at 16600\n",
      "Start ptr is at 16700\n",
      "Start ptr is at 16800\n",
      "Start ptr is at 16900\n",
      "Start ptr is at 17000\n",
      "Start ptr is at 17100\n",
      "Start ptr is at 17200\n",
      "Start ptr is at 17300\n",
      "Start ptr is at 17400\n",
      "Start ptr is at 17500\n",
      "Start ptr is at 17600\n",
      "Start ptr is at 17700\n",
      "Start ptr is at 17800\n",
      "Start ptr is at 17900\n",
      "Start ptr is at 18000\n",
      "Start ptr is at 18100\n",
      "Start ptr is at 18200\n",
      "Start ptr is at 18300\n",
      "Start ptr is at 18400\n",
      "Start ptr is at 18500\n",
      "Start ptr is at 18600\n",
      "Start ptr is at 18700\n",
      "Start ptr is at 18800\n",
      "Start ptr is at 18900\n",
      "Start ptr is at 19000\n",
      "Start ptr is at 19100\n",
      "Start ptr is at 19200\n",
      "Start ptr is at 19300\n",
      "Start ptr is at 19400\n",
      "Start ptr is at 19500\n",
      "Start ptr is at 19600\n",
      "Start ptr is at 19700\n",
      "Start ptr is at 19800\n",
      "Start ptr is at 19900\n",
      "Start ptr is at 20000\n",
      "Start ptr is at 20100\n",
      "Start ptr is at 20200\n",
      "Start ptr is at 20300\n",
      "Start ptr is at 20400\n",
      "Start ptr is at 20500\n",
      "Start ptr is at 20600\n",
      "Start ptr is at 20700\n",
      "Start ptr is at 20800\n",
      "Start ptr is at 20900\n",
      "Start ptr is at 21000\n",
      "Start ptr is at 21100\n",
      "Start ptr is at 21200\n",
      "Start ptr is at 21300\n",
      "Start ptr is at 21400\n",
      "Start ptr is at 21500\n",
      "Start ptr is at 21600\n",
      "Start ptr is at 21700\n",
      "Start ptr is at 21800\n",
      "Start ptr is at 21900\n",
      "Start ptr is at 22000\n",
      "Start ptr is at 22100\n",
      "Start ptr is at 22200\n",
      "Start ptr is at 22300\n",
      "Start ptr is at 22400\n",
      "Start ptr is at 22500\n",
      "Start ptr is at 22600\n",
      "Start ptr is at 22700\n",
      "Start ptr is at 22800\n",
      "Start ptr is at 22900\n",
      "Start ptr is at 23000\n",
      "Start ptr is at 23100\n",
      "Start ptr is at 23200\n",
      "Start ptr is at 23300\n",
      "Start ptr is at 23400\n",
      "Start ptr is at 23500\n",
      "Start ptr is at 23600\n",
      "Start ptr is at 23700\n",
      "Start ptr is at 23800\n",
      "Start ptr is at 23900\n",
      "Start ptr is at 24000\n",
      "Start ptr is at 24100\n",
      "Start ptr is at 24200\n",
      "Start ptr is at 24300\n",
      "Start ptr is at 24400\n",
      "Start ptr is at 24500\n",
      "Start ptr is at 24600\n",
      "Start ptr is at 24700\n",
      "Start ptr is at 24800\n",
      "Start ptr is at 24900\n",
      "Start ptr is at 25000\n",
      "Start ptr is at 25100\n",
      "Start ptr is at 25200\n",
      "Start ptr is at 25300\n",
      "Start ptr is at 25400\n",
      "Start ptr is at 25500\n",
      "Start ptr is at 25600\n",
      "Start ptr is at 25700\n",
      "Start ptr is at 25800\n",
      "Start ptr is at 25900\n",
      "Start ptr is at 26000\n",
      "Start ptr is at 26100\n",
      "Start ptr is at 26200\n",
      "Start ptr is at 26300\n",
      "Start ptr is at 26400\n",
      "Start ptr is at 26500\n",
      "Start ptr is at 26600\n",
      "Start ptr is at 26700\n",
      "Start ptr is at 26800\n",
      "Start ptr is at 26900\n",
      "Start ptr is at 27000\n",
      "Start ptr is at 27100\n",
      "Start ptr is at 27200\n",
      "Start ptr is at 27300\n",
      "Start ptr is at 27400\n",
      "Start ptr is at 27500\n",
      "Start ptr is at 27600\n",
      "Start ptr is at 27700\n",
      "Start ptr is at 27800\n",
      "Start ptr is at 27900\n",
      "Start ptr is at 28000\n",
      "Start ptr is at 28100\n",
      "Start ptr is at 28200\n",
      "Start ptr is at 28300\n",
      "Start ptr is at 28400\n",
      "Start ptr is at 28500\n",
      "Start ptr is at 28600\n",
      "Start ptr is at 28700\n",
      "Start ptr is at 28800\n",
      "Start ptr is at 28900\n",
      "Start ptr is at 29000\n",
      "Start ptr is at 29100\n",
      "Start ptr is at 29200\n",
      "Start ptr is at 29300\n",
      "Start ptr is at 29400\n",
      "Start ptr is at 29500\n",
      "Start ptr is at 29600\n",
      "Start ptr is at 29700\n",
      "Start ptr is at 29800\n",
      "Start ptr is at 29900\n",
      "Start ptr is at 30000\n",
      "Start ptr is at 30100\n",
      "Start ptr is at 30200\n",
      "Start ptr is at 30300\n",
      "Start ptr is at 30400\n",
      "Start ptr is at 30500\n",
      "Start ptr is at 30600\n",
      "Start ptr is at 30700\n",
      "Start ptr is at 30800\n",
      "Start ptr is at 30900\n",
      "Start ptr is at 31000\n",
      "Start ptr is at 31100\n",
      "Start ptr is at 31200\n",
      "Start ptr is at 31300\n",
      "Start ptr is at 31400\n",
      "Start ptr is at 31500\n",
      "Start ptr is at 31600\n",
      "Start ptr is at 31700\n",
      "Start ptr is at 31800\n",
      "Start ptr is at 31900\n",
      "Start ptr is at 32000\n",
      "Start ptr is at 32100\n",
      "Start ptr is at 32200\n",
      "Start ptr is at 32300\n",
      "Start ptr is at 32400\n",
      "Start ptr is at 32500\n",
      "Start ptr is at 32600\n",
      "Start ptr is at 32700\n",
      "Start ptr is at 32800\n",
      "Start ptr is at 32900\n",
      "Start ptr is at 33000\n",
      "Start ptr is at 33100\n",
      "Start ptr is at 33200\n",
      "Start ptr is at 33300\n",
      "Start ptr is at 33400\n",
      "Start ptr is at 33500\n",
      "Start ptr is at 33600\n",
      "Start ptr is at 33700\n",
      "Start ptr is at 33800\n",
      "Start ptr is at 33900\n",
      "Start ptr is at 34000\n",
      "Start ptr is at 34100\n",
      "Start ptr is at 34200\n",
      "Start ptr is at 34300\n",
      "Start ptr is at 34400\n",
      "Start ptr is at 34500\n",
      "Start ptr is at 34600\n",
      "Start ptr is at 34700\n",
      "Start ptr is at 34800\n",
      "Start ptr is at 34900\n",
      "Start ptr is at 35000\n",
      "Start ptr is at 35100\n",
      "Start ptr is at 35200\n",
      "Start ptr is at 35300\n",
      "Start ptr is at 35400\n",
      "Start ptr is at 35500\n",
      "Start ptr is at 35600\n",
      "Start ptr is at 35700\n",
      "Start ptr is at 35800\n",
      "Start ptr is at 35900\n",
      "Start ptr is at 36000\n",
      "Start ptr is at 36100\n",
      "Start ptr is at 36200\n",
      "Start ptr is at 36300\n",
      "Start ptr is at 36400\n",
      "Start ptr is at 36500\n",
      "Start ptr is at 36600\n",
      "Start ptr is at 36700\n",
      "Start ptr is at 36800\n",
      "Start ptr is at 36900\n",
      "Start ptr is at 37000\n",
      "Start ptr is at 37100\n",
      "Start ptr is at 37200\n",
      "Start ptr is at 37300\n",
      "Start ptr is at 37400\n",
      "Start ptr is at 37500\n",
      "Start ptr is at 37600\n",
      "Start ptr is at 37700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start ptr is at 37800\n",
      "Start ptr is at 37900\n",
      "Start ptr is at 38000\n",
      "Start ptr is at 38100\n",
      "Start ptr is at 38200\n",
      "Start ptr is at 38300\n",
      "Start ptr is at 38400\n",
      "Start ptr is at 38500\n",
      "Start ptr is at 38600\n",
      "Start ptr is at 38700\n",
      "Start ptr is at 38800\n",
      "Start ptr is at 38900\n",
      "Start ptr is at 39000\n",
      "Start ptr is at 39100\n",
      "Start ptr is at 39200\n",
      "Start ptr is at 39300\n",
      "Start ptr is at 39400\n",
      "Start ptr is at 39500\n",
      "Start ptr is at 39600\n",
      "Start ptr is at 39700\n",
      "Start ptr is at 39800\n",
      "Start ptr is at 39900\n",
      "Start ptr is at 40000\n",
      "Start ptr is at 40100\n",
      "Start ptr is at 40200\n",
      "Start ptr is at 40300\n",
      "Start ptr is at 40400\n",
      "Start ptr is at 40500\n",
      "Start ptr is at 40600\n",
      "Start ptr is at 40700\n",
      "Start ptr is at 40800\n",
      "Start ptr is at 40900\n",
      "Start ptr is at 41000\n",
      "Start ptr is at 41100\n",
      "Start ptr is at 41200\n",
      "Start ptr is at 41300\n",
      "Start ptr is at 41400\n",
      "Start ptr is at 41500\n",
      "Start ptr is at 41600\n",
      "Start ptr is at 41700\n",
      "Start ptr is at 41800\n",
      "Start ptr is at 41900\n",
      "Start ptr is at 42000\n",
      "Start ptr is at 42100\n",
      "Start ptr is at 42200\n",
      "Start ptr is at 42300\n",
      "Start ptr is at 42400\n",
      "Start ptr is at 42500\n",
      "Start ptr is at 42600\n",
      "Start ptr is at 42700\n",
      "Start ptr is at 42800\n",
      "Start ptr is at 42900\n",
      "Start ptr is at 43000\n",
      "Start ptr is at 43100\n",
      "Start ptr is at 43200\n",
      "Start ptr is at 43300\n",
      "Start ptr is at 43400\n",
      "Start ptr is at 43500\n",
      "Start ptr is at 43600\n",
      "Start ptr is at 43700\n",
      "Start ptr is at 43800\n",
      "Start ptr is at 43900\n",
      "Start ptr is at 44000\n",
      "Start ptr is at 44100\n",
      "Start ptr is at 44200\n",
      "Start ptr is at 44300\n",
      "Start ptr is at 44400\n",
      "Start ptr is at 44500\n",
      "Start ptr is at 44600\n",
      "Start ptr is at 44700\n",
      "Start ptr is at 44800\n",
      "Start ptr is at 44900\n",
      "Start ptr is at 45000\n",
      "Start ptr is at 45100\n",
      "Start ptr is at 45200\n",
      "Start ptr is at 45300\n",
      "Start ptr is at 45400\n",
      "Start ptr is at 45500\n",
      "Start ptr is at 45600\n",
      "Start ptr is at 45700\n",
      "Start ptr is at 45800\n",
      "Start ptr is at 45900\n",
      "Start ptr is at 46000\n",
      "Start ptr is at 46100\n",
      "Start ptr is at 46200\n",
      "Start ptr is at 46300\n",
      "Start ptr is at 46400\n",
      "Start ptr is at 46500\n",
      "Start ptr is at 46600\n",
      "Start ptr is at 46700\n",
      "Start ptr is at 46800\n",
      "Start ptr is at 46900\n",
      "Start ptr is at 47000\n",
      "Start ptr is at 47100\n",
      "Start ptr is at 47200\n",
      "Start ptr is at 47300\n",
      "Start ptr is at 47400\n",
      "Start ptr is at 47500\n",
      "Start ptr is at 47600\n",
      "Start ptr is at 47700\n",
      "Start ptr is at 47800\n",
      "Start ptr is at 47900\n",
      "Start ptr is at 48000\n",
      "Start ptr is at 48100\n",
      "Start ptr is at 48200\n",
      "Start ptr is at 48300\n",
      "Start ptr is at 48400\n",
      "Start ptr is at 48500\n",
      "Start ptr is at 48600\n",
      "Start ptr is at 48700\n",
      "Start ptr is at 48800\n",
      "Start ptr is at 48900\n",
      "Start ptr is at 49000\n",
      "Start ptr is at 49100\n",
      "Start ptr is at 49200\n",
      "Start ptr is at 49300\n",
      "Start ptr is at 49400\n",
      "Start ptr is at 49500\n",
      "Start ptr is at 49600\n",
      "Start ptr is at 49700\n",
      "Start ptr is at 49800\n",
      "Start ptr is at 49900\n",
      "Start ptr is at 50000\n",
      "Start ptr is at 50100\n",
      "Start ptr is at 50200\n",
      "Start ptr is at 50300\n",
      "Start ptr is at 50400\n",
      "Start ptr is at 50500\n",
      "Start ptr is at 50600\n",
      "Start ptr is at 50700\n",
      "Start ptr is at 50800\n",
      "Start ptr is at 50900\n",
      "Start ptr is at 51000\n",
      "Start ptr is at 51100\n",
      "Start ptr is at 51200\n",
      "Start ptr is at 51300\n",
      "Start ptr is at 51400\n",
      "Start ptr is at 51500\n",
      "Start ptr is at 51600\n",
      "Start ptr is at 51700\n",
      "Start ptr is at 51800\n",
      "Start ptr is at 51900\n",
      "Start ptr is at 52000\n",
      "Start ptr is at 52100\n",
      "Start ptr is at 52200\n",
      "Start ptr is at 52300\n",
      "Start ptr is at 52400\n",
      "Start ptr is at 52500\n",
      "Start ptr is at 52600\n",
      "Start ptr is at 52700\n",
      "Start ptr is at 52800\n",
      "Start ptr is at 52900\n",
      "Start ptr is at 53000\n",
      "Start ptr is at 53100\n",
      "Start ptr is at 53200\n",
      "Start ptr is at 53300\n",
      "Start ptr is at 53400\n",
      "Start ptr is at 53500\n",
      "Start ptr is at 53600\n",
      "Start ptr is at 53700\n",
      "Start ptr is at 53800\n",
      "Start ptr is at 53900\n",
      "Start ptr is at 54000\n",
      "Start ptr is at 54100\n",
      "Start ptr is at 54200\n",
      "Start ptr is at 54300\n",
      "Start ptr is at 54400\n",
      "Start ptr is at 54500\n",
      "Start ptr is at 54600\n",
      "Start ptr is at 54700\n",
      "Start ptr is at 54800\n",
      "Start ptr is at 54900\n",
      "Start ptr is at 55000\n",
      "Start ptr is at 55100\n",
      "Start ptr is at 55200\n",
      "Start ptr is at 55300\n",
      "Start ptr is at 55400\n",
      "Start ptr is at 55500\n",
      "Start ptr is at 55600\n",
      "Start ptr is at 55700\n",
      "Start ptr is at 55800\n",
      "Start ptr is at 55900\n",
      "Start ptr is at 56000\n",
      "Start ptr is at 56100\n",
      "Start ptr is at 56200\n",
      "Start ptr is at 56300\n",
      "Start ptr is at 56400\n",
      "Start ptr is at 56500\n",
      "Start ptr is at 56600\n",
      "Start ptr is at 56700\n",
      "Start ptr is at 56800\n",
      "Start ptr is at 56900\n",
      "Start ptr is at 57000\n",
      "Start ptr is at 57100\n",
      "Start ptr is at 57200\n",
      "Start ptr is at 57300\n",
      "Start ptr is at 57400\n",
      "Start ptr is at 57500\n",
      "Start ptr is at 57600\n",
      "Start ptr is at 57700\n",
      "Start ptr is at 57800\n",
      "Start ptr is at 57900\n",
      "Start ptr is at 58000\n",
      "Start ptr is at 58100\n",
      "Start ptr is at 58200\n",
      "Start ptr is at 58300\n",
      "Start ptr is at 58400\n",
      "Start ptr is at 58500\n",
      "Start ptr is at 58600\n",
      "Start ptr is at 58700\n",
      "Start ptr is at 58800\n",
      "Start ptr is at 58900\n",
      "Start ptr is at 59000\n",
      "Start ptr is at 59100\n",
      "Start ptr is at 59200\n",
      "Start ptr is at 59300\n",
      "Start ptr is at 59400\n",
      "Start ptr is at 59500\n",
      "Start ptr is at 59600\n",
      "Start ptr is at 59700\n",
      "Start ptr is at 59800\n",
      "Start ptr is at 59900\n",
      "Start ptr is at 60000\n",
      "Start ptr is at 60100\n",
      "Start ptr is at 60200\n",
      "Start ptr is at 60300\n",
      "Start ptr is at 60400\n",
      "Start ptr is at 60500\n",
      "Start ptr is at 60600\n",
      "Start ptr is at 60700\n",
      "Start ptr is at 60800\n",
      "Start ptr is at 60900\n",
      "Start ptr is at 61000\n",
      "Start ptr is at 61100\n",
      "Start ptr is at 61200\n",
      "Start ptr is at 61300\n",
      "Start ptr is at 61400\n",
      "Start ptr is at 61500\n",
      "Start ptr is at 61600\n",
      "Start ptr is at 61700\n",
      "Start ptr is at 61800\n",
      "Start ptr is at 61900\n",
      "Start ptr is at 62000\n",
      "Start ptr is at 62100\n",
      "Start ptr is at 62200\n",
      "Start ptr is at 62300\n",
      "Start ptr is at 62400\n",
      "Start ptr is at 62500\n",
      "Start ptr is at 62600\n",
      "Start ptr is at 62700\n",
      "Start ptr is at 62800\n",
      "Start ptr is at 62900\n",
      "Start ptr is at 63000\n",
      "Start ptr is at 63100\n",
      "Start ptr is at 63200\n",
      "Start ptr is at 63300\n",
      "Start ptr is at 63400\n",
      "Start ptr is at 63500\n",
      "Start ptr is at 63600\n",
      "Start ptr is at 63700\n",
      "Start ptr is at 63800\n",
      "Start ptr is at 63900\n",
      "Start ptr is at 64000\n",
      "Start ptr is at 64100\n"
     ]
    }
   ],
   "source": [
    "#collected tweets from tweepy \n",
    "start_ptr = 0\n",
    "end_ptr = start_ptr + 100 \n",
    "replies = {}\n",
    "status_ids = [output['in_reply_to_status_id'] for output in outputs]\n",
    "print(len(status_ids))\n",
    "count = 0\n",
    "save_count = 0\n",
    "while(start_ptr < len(status_ids)):\n",
    "    print(\"Start ptr is at {}\".format(start_ptr))\n",
    "    reply_tweets = api.statuses_lookup(status_ids[start_ptr:end_ptr],trim_user=True)\n",
    "#     if count % 1000 == 0:\n",
    "#         print(\"Saving at {}\".format(start_ptr))\n",
    "#         np.savez(os.path.join(ROOT_DIR, 'data/reply_tweets{}.npz'.format(save_count)), a=replies)\n",
    "#         save_count += 1\n",
    "#         replies = {}\n",
    "#     # load and save results each time \n",
    "#     if start_ptr != 0:\n",
    "#         replies = np.load(os.path.join(ROOT_DIR, 'data/reply_data.npy'))\n",
    "#         replies = replies[()]\n",
    "    for i, reply_tweet in enumerate(reply_tweets):\n",
    "        reply_tweet = reply_tweet._json\n",
    "        replies[reply_tweet['id']] = reply_tweet['text']\n",
    "        count += 1\n",
    "        \n",
    "    start_ptr += 100 \n",
    "    end_ptr += 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10270"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(os.path.join(ROOT_DIR, 'data/reply_data.npz'), a=replies)\n",
    "replies = np.load(os.path.join(ROOT_DIR, 'data/reply_data.npy'))\n",
    "replies = replies[()]\n",
    "len(replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count = 0 \n",
    "# 0.16608010725657785 deleted... \n",
    "for status_id in status_ids:\n",
    "    if str(status_id) not in replies:\n",
    "        count += 1\n",
    "print(count / len(status_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verifies missing tweet \n",
    "# missing_tweet = api.statuses_lookup([847652506372984835],trim_user=True)\n",
    "# missing_tweet\n",
    "hey = '         '\n",
    "hey.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining tweets \n",
    "status_ids_fetched = []\n",
    "outputs_context = []\n",
    "for output in outputs:\n",
    "    status_id = str(output['in_reply_to_status_id'])\n",
    "    if status_id in replies:\n",
    "        output['reply_to_tweet_text'] = output['text'] + replies[status_id]\n",
    "    else:\n",
    "        output['reply_to_tweet_text'] = output['text'] + output['text']\n",
    "    outputs_context.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "\n",
    "def tokenize(outpus):\n",
    "    key = 'reply_to_tweet_text'\n",
    "    outputs_processed = []\n",
    "    for output in outputs:\n",
    "        text = output[key]\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        tokens = text.split(' ')\n",
    "        output['tokens'] = tokens\n",
    "        outputs_processed.append(output)\n",
    "    return outputs_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_processed = tokenize(outputs_context)\n",
    "outputs_processed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sizes] Training set: 64.00%, Validation set: 16.00%, Test set: 20.00%\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_valid, y_valid, x_test, y_test = split_data(outputs, labels, 28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_EMBED_DIM = 300\n",
    "TWITTER_EMBED_DIM = 400\n",
    "TWEET_SENTENCE_SIZE = 17*2 # 16 is average tweet token length\n",
    "TWEET_WORD_SIZE = 20 # selected by histogram of tweet counts\n",
    "FASTTEXT_EMBED_DIM = 300\n",
    "EMBED_DIM = 200\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "def generate_random_embedding(embed_dim):\n",
    "    return np.random.normal(scale=0.6, size=(embed_dim,))\n",
    "\n",
    "def process_tweet(tweet, embed_dim, word_vectors):\n",
    "    embedded_tweet = []\n",
    "\n",
    "    # trim if too large\n",
    "    if len(tweet) >= TWEET_SENTENCE_SIZE:\n",
    "        tweet = tweet[:TWEET_SENTENCE_SIZE]\n",
    "\n",
    "    # convert all into word embeddings\n",
    "    for word in tweet:\n",
    "        embedding = generate_random_embedding(embed_dim) if word not in word_vectors else word_vectors[word]\n",
    "        embedded_tweet.append(embedding)\n",
    "\n",
    "    # pad if too short\n",
    "    if len(tweet) < TWEET_SENTENCE_SIZE:\n",
    "        diff = TWEET_SENTENCE_SIZE - len(tweet)\n",
    "        embedded_tweet += [generate_random_embedding(embed_dim) for _ in range(diff)]\n",
    "    return embedded_tweet\n",
    "\n",
    "# embeds tokens! \n",
    "def fetch_word_embeddings(outputs, word_vectors, embed_dim, experiment_flag):\n",
    "    outputs_embed = []\n",
    "    for i, output in enumerate(outputs):\n",
    "\n",
    "        # process first tweet\n",
    "        embedded_tweet = process_tweet(output['tokens'], embed_dim, word_vectors)\n",
    "\n",
    "        if experiment_flag == 2:\n",
    "            #proceses second tweet\n",
    "            if output['context_tweet'] is None:\n",
    "                for i in range(TWEET_SENTENCE_SIZE):\n",
    "                    blank_embedding = np.zeros(embed_dim,)\n",
    "                    embedded_tweet.append(blank_embedding)\n",
    "            else:\n",
    "                context_embedding = process_tweet(output['context_tokens'], embed_dim, word_vectors)\n",
    "                for i in range(TWEET_SENTENCE_SIZE):\n",
    "                    embedded_tweet.append(context_embedding[i])\n",
    "            assert len(embedded_tweet) == TWEET_SENTENCE_SIZE*2\n",
    "\n",
    "        # add real numbers \n",
    "        if experiment_flag == 3:\n",
    "            for i, embed in enumerate(embedded_tweet):\n",
    "                print(np.array(embed).shape)\n",
    "                embedded_tweet[i] = np.concatenate((embed, [output['retweet_count'], output['favorite_count']]))\n",
    "                \n",
    "        output['embedding'] = embedded_tweet\n",
    "        print(np.array(embedded_tweet).shape)\n",
    "        break\n",
    "        outputs_embed.append(output)\n",
    "    return outputs_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashemagalhaes/miniconda3/envs/mlp/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time 2.192983249823252 min\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "embed_dim = 400\n",
    "filename = os.path.join(ROOT_DIR, 'data/word2vec_twitter_model/word2vec_twitter_model.bin')\n",
    "word_vectors = KeyedVectors.load_word2vec_format(filename, binary=True, unicode_errors='ignore')\n",
    "print(\"Total time {} min\".format((time.time() - start) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(400,)\n",
      "(34, 402)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "x_train_embed = fetch_word_embeddings(x_train, word_vectors, embed_dim, 3)\n",
    "# x_valid_embed = fetch_word_embeddings(x_valid, word_vectors, embed_dim)\n",
    "# x_test_embed = fetch_word_embeddings(x_test, word_vectors, embed_dim)\n",
    "# print(\"Total time {} min\".format((time.time() - start) / 60))\n",
    "# print(len(x_train_embed), len(x_valid_embed), len(x_test_embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment flag 3\n",
      "Experiment flag 3\n",
      "Experiment flag 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def convert_to_feature_embeddings(x_embed, experiment_flag, key='embedding'):\n",
    "    \"\"\"\n",
    "    :param x_embed: dictionary with all params of processed tweets\n",
    "    :param key: what params should be kept\n",
    "    :param experiment_flag: which type of experiment\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print(\"Experiment flag {}\".format(experiment_flag))\n",
    "    if key == 'tokens': #  for tdidf\n",
    "        if experiment_flag == 1:\n",
    "            return [x['tweet'] for x in x_embed]\n",
    "        else:\n",
    "            output = []\n",
    "            for x in x_embed:\n",
    "                if x['context_tweet']:\n",
    "                    output.append(x['tweet'] + '\\n' + x['context_tweet'])\n",
    "                else:\n",
    "                    output.append(x['tweet'] + '\\n' + ' '.join([' '] * len(x['tweet'])))\n",
    "            return output\n",
    "    return [x[key] for x in x_embed]\n",
    "\n",
    "vectorizer = TfidfVectorizer(use_idf=True, max_features=10000)\n",
    "x_train_embed = vectorizer.fit_transform(convert_to_feature_embeddings(x_train,\n",
    "                                                                       key='tokens',\n",
    "                                                                       experiment_flag=3\n",
    "                                                                       )).todense()\n",
    "x_valid_embed = vectorizer.transform(convert_to_feature_embeddings(x_valid,\n",
    "                                                                   key='tokens',\n",
    "                                                                   experiment_flag=3\n",
    "                                                                   )).todense()\n",
    "x_test_embed = vectorizer.transform(convert_to_feature_embeddings(x_test,\n",
    "                                                                  key='tokens',\n",
    "                                                                          experiment_flag=3\n",
    "                                                                          )).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41055, 10002)\n"
     ]
    }
   ],
   "source": [
    "def generate_tdidf_experiment_3_embeddings(data_list, embeddings):\n",
    "    processed_embeddings = []\n",
    "    for i, embed in enumerate(embeddings):\n",
    "        embed = np.transpose(np.array(embed))\n",
    "        embed = embed.reshape(-1)\n",
    "        output = data_list[i]\n",
    "        processed_embeddings.append(np.concatenate((embed, [output['retweet_count'], output['favorite_count']])))\n",
    "    return processed_embeddings\n",
    "\n",
    "x_train_embed = generate_tdidf_experiment_3_embeddings(x_train, x_train_embed)\n",
    "print(np.array(x_train_embed).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_embed[0].keys() # list of dictionaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0] #int "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17 n-gram \n",
    "# retweet count \n",
    "# in reply to status id \n",
    "# favorite count \n",
    "\n",
    "# 34 n-gram \n",
    "def convert_to_feature_embeddings(x_embed):\n",
    "    return [x['word_embeddings'] for x in x_embed]\n",
    " \n",
    "data = {}\n",
    "print(x_train_embed[0].keys())\n",
    "data['x_train'] = convert_to_feature_embeddings(x_train_embed)\n",
    "data['y_train'] = y_train\n",
    "data['x_valid'] = convert_to_feature_embeddings(x_valid_embed)\n",
    "data['y_valid'] = y_valid\n",
    "data['x_test'] = convert_to_feature_embeddings(x_test_embed)\n",
    "data['y_test'] = y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "def wrap_data(batch_size, seed, x_train, y_train, x_valid, y_valid, x_test, y_test):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    transform=None\n",
    "    \n",
    "    train_set = DataProvider(inputs=x_train, targets=y_train, seed=seed, transform=transform)\n",
    "    train_data_local = torch.utils.data.DataLoader(train_set,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   num_workers=2,\n",
    "                                                   sampler=ImbalancedDatasetSampler(train_set),\n",
    "                                                   )\n",
    "\n",
    "    valid_set = DataProvider(inputs=x_valid, targets=y_valid, seed=seed, transform=transform)\n",
    "    valid_data_local = torch.utils.data.DataLoader(valid_set,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   num_workers=2,\n",
    "                                                   shuffle=False,\n",
    "                                                  )\n",
    "\n",
    "    test_set = DataProvider(inputs=x_test, targets=y_test, seed=seed, transform=transform)\n",
    "    test_data_local = torch.utils.data.DataLoader(test_set,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  num_workers=2,\n",
    "                                                  shuffle=False,\n",
    "                                                 )\n",
    "    return train_data_local, valid_data_local, test_data_local\n",
    "\n",
    "def fetch_model(model, embedding_level, input_shape_local, dropout):\n",
    "    if model == 'MLP':\n",
    "        return multi_layer_perceptron(input_shape_local)\n",
    "    if model == 'CNN':\n",
    "        if embedding_level == 'word':\n",
    "            return word_cnn(input_shape_local, dropout)\n",
    "        elif embedding_level == 'character':\n",
    "            return character_cnn(input_shape_local)\n",
    "    if model == 'DENSENET':\n",
    "        return densenet()\n",
    "    else:\n",
    "        raise ValueError(\"Model key not found {}\".format(embedding_level))\n",
    "\n",
    "\n",
    "def fetch_model_parameters(input_shape_local):\n",
    "    model_local = fetch_model(model='CNN',\n",
    "                            embedding_level='word',\n",
    "                            input_shape_local=input_shape_local,\n",
    "                            dropout=0.5)\n",
    "    criterion_local = torch.nn.CrossEntropyLoss()\n",
    "    optimizer_local = torch.optim.Adam(model_local.parameters(), weight_decay=1e-4)\n",
    "    scheduler_local = optim.lr_scheduler.CosineAnnealingLR(optimizer_local, T_max=100, eta_min=0.0001)\n",
    "    return model_local, criterion_local, optimizer_local, scheduler_local\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = wrap_data(2048, 28, **data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in train_data:\n",
    "    input_shape = x.shape\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = tuple(input_shape)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train_iter(model, device, optimizer, criterion, x, y, stats, experiment_key='train'):\n",
    "    \"\"\"\n",
    "    Receives the inputs and targets for the model and runs a training iteration. Returns loss and accuracy metrics.\n",
    "    :param x: The inputs to the model. A numpy array of shape batch_size, channels, height, width\n",
    "    :param y: The targets for the model. A numpy array of shape batch_size, num_classes\n",
    "    :return: the loss and accuracy for this batch\n",
    "    \"\"\"\n",
    "    # sets model to training mode\n",
    "    # (in case batch normalization or other methods have different procedures for training and evaluation)\n",
    "    model.train()\n",
    "    x = x.float()\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    optimizer.zero_grad()  # set all weight grads from previous training iters to 0\n",
    "    out = model.forward(x)  # forward the data in the model\n",
    "    # loss = F.cross_entropy(input=out, target=y)  # compute loss\n",
    "    loss = criterion(out, y)\n",
    "    loss.backward()  # backpropagate to compute gradients for current iter loss\n",
    "\n",
    "    optimizer.step()  # update network parameters\n",
    "    _, predicted = torch.max(out.data, 1)  # get argmax of predictions\n",
    "    accuracy = np.mean(list(predicted.eq(y.data).cpu()))  # compute accuracy\n",
    "    stats['{}_acc'.format(experiment_key)].append(accuracy)\n",
    "    stats['{}_loss'.format(experiment_key)].append(loss.data.detach().cpu().numpy())\n",
    "\n",
    "def run_evaluation_iter(model, device, optimizer, criterion, x, y, stats, experiment_key='valid'):\n",
    "    \"\"\"\n",
    "    Receives the inputs and targets for the model and runs an evaluation iterations. Returns loss and accuracy metrics.\n",
    "    :param x: The inputs to the model. A numpy array of shape batch_size, channels, height, width\n",
    "    :param y: The targets for the model. A numpy array of shape batch_size, num_classes\n",
    "    :return: the loss and accuracy for this batch\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        model.eval()  # sets the system to validation mode\n",
    "        x = x.float()\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        out = model.forward(x)  # forward the data in the model\n",
    "        loss = criterion(out, y)\n",
    "        \n",
    "        # loss = F.cross_entropy(out, y)  # compute loss\n",
    "        _, predicted = torch.max(out.data, 1)  # get argmax of predictions\n",
    "        \n",
    "        accuracy = np.mean(list(predicted.eq(y.data).cpu()))\n",
    "        stats['{}_acc'.format(experiment_key)].append(accuracy)  # compute accuracy\n",
    "        stats['{}_loss'.format(experiment_key)].append(loss.data.detach().cpu().numpy())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_save_dir, model_save_name, model_idx):\n",
    "    \"\"\"\n",
    "    Save the network parameter state and current best val epoch idx and best val accuracy.\n",
    "    :param model_save_name: Name to use to save model without the epoch index\n",
    "    :param model_idx: The index to save the model with.\n",
    "    :param best_validation_model_idx: The index of the best validation model to be stored for future use.\n",
    "    :param best_validation_model_acc: The best validation accuracy to be stored for use at test time.\n",
    "    :param model_save_dir: The directory to store the state at.\n",
    "    :param state: The dictionary containing the system state.\n",
    "\n",
    "    \"\"\"\n",
    "    # Save state each epoch\n",
    "    path = os.path.join(model_save_dir, \"{}_{}\".format(model_save_name, str(model_idx)))\n",
    "    torch.save(model.state_dict(), f=path)\n",
    "    \n",
    "\n",
    "def load_model(model, model_save_dir, model_save_name, model_idx):\n",
    "    \"\"\"\n",
    "    Load the network parameter state and the best val model idx and best val acc to be compared with the future val accuracies, in order to choose the best val model\n",
    "    :param model_save_dir: The directory to store the state at.\n",
    "    :param model_save_name: Name to use to save model without the epoch index\n",
    "    :param model_idx: The index to save the model with.\n",
    "    \"\"\"\n",
    "    path = os.path.join(model_save_dir, \"{}_{}\".format(model_save_name, str(model_idx)))\n",
    "    checkpoint = torch.load(f=path)\n",
    "    # freeze parameters\n",
    "    model.load_state_dict(checkpoint)\n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict, defaultdict\n",
    "import tqdm\n",
    "\n",
    "model, criterion, optimizer, _ = fetch_model_parameters(input_shape)\n",
    "device = torch.device('cpu')\n",
    "train_stats = OrderedDict()\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    epoch_stats = defaultdict(list)\n",
    "    with tqdm.tqdm(total=len(train_data)) as pbar_train:  # create a progress bar for training\n",
    "        for idx, (x, y) in enumerate(train_data):  # get data batches\n",
    "            run_train_iter(model, device, optimizer, criterion, x=x, y=y, stats=epoch_stats)  # take a training iter step\n",
    "            pbar_train.update(1)\n",
    "            pbar_train.set_description(\"loss: {:.4f}, accuracy: {:.4f}\".format(epoch_stats['train_loss'][-1],\n",
    "                                                                               epoch_stats['train_acc'][-1]))\n",
    "\n",
    "    with tqdm.tqdm(total=len(valid_data)) as pbar_val:  # create a progress bar for validation\n",
    "        for x, y in valid_data:  # get data batches\n",
    "            run_evaluation_iter(model, device, optimizer, criterion, x=x, y=y, stats=epoch_stats)  # run a validation iter\n",
    "            pbar_val.update(1)  # add 1 step to the progress bar\n",
    "            pbar_val.set_description(\"loss: {:.4f}, accuracy: {:.4f}\".format(epoch_stats['valid_loss'][-1],\n",
    "                                                                             epoch_stats['valid_acc'][-1]))\n",
    "     \n",
    "    \n",
    "    \n",
    "    save_model(model, '', 'testing', epoch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model(model, '', 'testing', 1)\n",
    "\n",
    "#evaluate test here\n",
    "with tqdm.tqdm(total=len(test_data)) as pbar_test:  # create a progress bar for validation\n",
    "    for x, y in test_data:  # get data batches\n",
    "        run_evaluation_iter(model, device, optimizer, criterion, x=x, y=y, stats=epoch_stats, experiment_key=\"test_experiment\")  # run a validation iter\n",
    "        pbar_test.update(1)  # add 1 step to the progress bar\n",
    "        pbar_test.set_description(\"loss: {:.4f}, accuracy: {:.4f}\".format(epoch_stats['test_experiment_loss'][-1],\n",
    "                                                                  epoch_stats['test_experiment_acc'][-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "for i in range(17):\n",
    "    arr.append(np.zeros(200,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(arr).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_tweet = np.array(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_tweet += np.array(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_tweet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_tweet = np.array(arr) + np.array(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_tweet = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(17):\n",
    "    blank_embedding = np.zeros(200,)\n",
    "    embedded_tweet.append(blank_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(embedded_tweet).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashemagalhaes/miniconda3/envs/mlp/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "_shape = (2048, 24) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.zeros(_shape)\n",
    "y = np.zeros(_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.Tensor(x)\n",
    "y = torch.Tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sol = torch.cat((x, y), 1)\n",
    "sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 48])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sol.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 24])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
