{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "from globals import ROOT_DIR\n",
    "from data_providers import TextDataProvider\n",
    "import argparse\n",
    "import configparser\n",
    "from torch import optim\n",
    "from experiment_builder import ExperimentBuilder\n",
    "from data_providers import *\n",
    "import os\n",
    "from models.cnn import *\n",
    "from models.multilayer_perceptron import multi_layer_perceptron\n",
    "import tweepy\n",
    "from utils import *\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../config.ini')\n",
    "path_data = os.path.join(ROOT_DIR, config['DEFAULT']['PATH_DATA'])\n",
    "path_labels = os.path.join(ROOT_DIR, config['DEFAULT']['PATH_LABELS'])\n",
    "\n",
    "consumer_key = config['DEFAULT']['TWITTER_CONSUMER_KEY']\n",
    "consumer_secret_key = config['DEFAULT']['TWITTER_CONSUMER_SECRET_KEY']\n",
    "access_token = config['DEFAULT']['TWITTER_ACCESS_TOKEN']\n",
    "access_token_secret = config['DEFAULT']['TWITTER_ACCESS_TOKEN_SECRET']\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret_key)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "data = np.load(os.path.join(ROOT_DIR, 'data/founta_data.npy'))\n",
    "data = data[()]\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fIoralhearts 2645\n",
      "Joe_Flores02 275\n",
      "Samiam3187 1300\n",
      "steve45220 841\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-0e1ed965d4dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mscreen_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'screen_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfollowers_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscreen_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0muser_followers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscreen_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlp/lib/python3.6/site-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlp/lib/python3.6/site-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m         data, cursors = self.method(cursor=self.next_cursor,\n\u001b[1;32m     74\u001b[0m                                     \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                                     **self.kargs)\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_cursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_cursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlp/lib/python3.6/site-packages/tweepy/binder.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;31m# Set pagination mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlp/lib/python3.6/site-packages/tweepy/binder.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    162\u001b[0m                                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_on_rate_limit_notify\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                                         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Rate limit reached. Sleeping for: %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msleep_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep_time\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# sleep for few extra sec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;31m# if self.wait_on_rate_limit and self._reset_time is not None and \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time \n",
    "user_followers = {}  \n",
    "count = 0 \n",
    "for key, value in data.items():\n",
    "    screen_name = value['user']['screen_name']\n",
    "    ids = []\n",
    "    for page in tweepy.Cursor(api.followers_ids, screen_name=screen_name).pages():\n",
    "        ids.extend(page)\n",
    "    user_followers[screen_name] = ids\n",
    "    print(\"{} {}\".format(screen_name, len(ids)))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2645\n",
      "275\n",
      "1300\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(embedding_key, embedding_level, seed, experiment_flag):\n",
    "    path_data = os.path.join(ROOT_DIR, config['DEFAULT']['PATH_DATA'])\n",
    "    path_labels = os.path.join(ROOT_DIR, config['DEFAULT']['PATH_LABELS'])\n",
    "    data_provider = TextDataProvider(path_data, path_labels, experiment_flag)\n",
    "\n",
    "    if embedding_level == 'word':\n",
    "        return data_provider.generate_word_level_embeddings(embedding_key, seed)\n",
    "    elif embedding_level == 'char':\n",
    "        output = data_provider.generate_char_level_embeddings(seed)\n",
    "    elif embedding_level == 'tdidf':\n",
    "        output = data_provider.generate_tdidf_embeddings(seed)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, data_map = extract_data('twitter', 'word', 28, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_data(batch_size, seed, data_local):\n",
    "    train_set = DataProvider(inputs=data_local['x_train'], targets=data_local['y_train'], seed=seed)\n",
    "\n",
    "    train_data_local = torch.utils.data.DataLoader(train_set,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   num_workers=2,\n",
    "                                                   sampler=ImbalancedDatasetSampler(train_set))\n",
    "    \n",
    "    valid_set = DataProvider(inputs=data_local['x_valid'], targets=data_local['y_valid'], seed=seed)\n",
    "    valid_data_local = torch.utils.data.DataLoader(valid_set,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   num_workers=2,\n",
    "                                                   shuffle=False)\n",
    "\n",
    "    test_set = DataProvider(inputs=data_local['x_test'], targets=data_local['y_test'], seed=seed)\n",
    "    test_data_local = torch.utils.data.DataLoader(test_set,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  num_workers=2,\n",
    "                                                  shuffle=False)\n",
    "\n",
    "    return train_data_local, valid_data_local, test_data_local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = wrap_data(2048, 28, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "out = torch.zeros([2048, 17, 400])\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.permute([0,2,1])\n",
    "out = F.max_pool1d(out, out.shape[-1])\n",
    "out = out.permute([0,2,1])\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_map['850023738779348994']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data['x_train'].keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['x_train']['847596956448960516']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data = pd.read_csv(os.path.join(ROOT_DIR, 'data/founta_data.csv'), header='infer', index_col=0, squeeze=True).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, labels = extract_tweets(label_data, data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid, x_test, y_test = split_data(list(outputs.keys()), labels, 28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf=True, max_features=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedded_tweets = vectorizer.fit_transform([outputs[key]['tweet'] for key in x_valid]).todense()\n",
    "embedded_context_tweets = vectorizer.fit_transform([outputs[key]['context_tweet'] if outputs[key]['context_tweet'] is not None else '' for key in x_valid ]).todense()\n",
    "for i, key in enumerate(x_valid):\n",
    "    outputs[key]['embedded_tweet'] = embedded_tweets[i]\n",
    "    outputs[key]['embedded_context_tweet'] = embedded_context_tweets[i]\n",
    "print(outputs[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in embedded_context_tweets:\n",
    "    tweet = tweet + np.zeros(100).T\n",
    "    print(len(tweet))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x_train_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_embed = np.array(x_train_embed).reshape(x_train_embed.shape[0], 1, x_train_embed.shape[1])\n",
    "x_train_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_key = 2\n",
    "data_provider = TextDataProvider(os.path.join(ROOT_DIR, 'data/founta_data.npy'), os.path.join(ROOT_DIR, 'data/founta_data.csv'), experiment_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_provider.generate_word_level_embeddings('bert', 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid, x_test, y_test = split_data(outputs, labels, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_embed = []\n",
    "for j, output in enumerate(x_train):\n",
    "    tweet_embed = data_provider.bert_embeddings[int(output['id'])]\n",
    "    reply_status_id = int(output['in_reply_to_status_id'])\n",
    "    blank_embed = []\n",
    "    if reply_status_id == -1 or reply_status_id not in data_provider.bert_embeddings:\n",
    "        for i in range(17):\n",
    "            blank_embedding = np.zeros(768, )\n",
    "            blank_embed.append(blank_embedding)\n",
    "        data_embed.append(tweet_embed + blank_embed)\n",
    "    else:\n",
    "        data_embed.append(tweet_embed + data_provider.bert_embeddings[reply_status_id])\n",
    "    if j > 10:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(data_embed).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.Tensor(data_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = embeddings[int(x_train[0]['id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(embed).reshape(1, embed[0], embed[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happens = 0\n",
    "data_embed = []\n",
    "no_longer_available = 0\n",
    "for output in outputs:\n",
    "    tweet_embed = embeddings[int(output['id'])]\n",
    "    reply_status_id = int(output['in_reply_to_status_id']) \n",
    "    final_embed = []\n",
    "    if reply_status_id != -1 and reply_status_id not in embeddings:\n",
    "        no_longer_available += 1\n",
    "    if reply_status_id == -1 or reply_status_id not in embeddings:\n",
    "        for i in range(17):\n",
    "            blank_embedding = np.zeros(768, )\n",
    "            final_embed.append(blank_embedding)\n",
    "        final_embed = final_embed + tweet_embed\n",
    "    else: \n",
    "        final_embed = final_embed + embeddings[reply_status_id]\n",
    "    data_embed.append(final_embed)\n",
    "    return data_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_embed[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_ids = [value['in_reply_to_status_id'] for key, value in data.items() if value['in_reply_to_status_id'] is not None]\n",
    "(len(status_ids) - len(replies)) / len(status_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collected tweets from tweepy \n",
    "start_ptr = 0\n",
    "end_ptr = start_ptr + 100 \n",
    "replies = {}\n",
    "status_ids = [output['in_reply_to_status_id'] for output in outputs]\n",
    "print(len(status_ids))\n",
    "count = 0\n",
    "save_count = 0\n",
    "while(start_ptr < len(status_ids)):\n",
    "    print(\"Start ptr is at {}\".format(start_ptr))\n",
    "    reply_tweets = api.statuses_lookup(status_ids[start_ptr:end_ptr],trim_user=True)\n",
    "#     if count % 1000 == 0:\n",
    "#         print(\"Saving at {}\".format(start_ptr))\n",
    "#         np.savez(os.path.join(ROOT_DIR, 'data/reply_tweets{}.npz'.format(save_count)), a=replies)\n",
    "#         save_count += 1\n",
    "#         replies = {}\n",
    "#     # load and save results each time \n",
    "#     if start_ptr != 0:\n",
    "#         replies = np.load(os.path.join(ROOT_DIR, 'data/reply_data.npy'))\n",
    "#         replies = replies[()]\n",
    "    for i, reply_tweet in enumerate(reply_tweets):\n",
    "        reply_tweet = reply_tweet._json\n",
    "        replies[reply_tweet['id']] = reply_tweet['text']\n",
    "        count += 1\n",
    "        \n",
    "    start_ptr += 100 \n",
    "    end_ptr += 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(os.path.join(ROOT_DIR, 'data/reply_data.npz'), a=replies)\n",
    "replies = np.load(os.path.join(ROOT_DIR, 'data/reply_data.npy'))\n",
    "replies = replies[()]\n",
    "len(replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count = 0 \n",
    "# 0.16608010725657785 deleted... \n",
    "for status_id in status_ids:\n",
    "    if str(status_id) not in replies:\n",
    "        count += 1\n",
    "print(count / len(status_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verifies missing tweet \n",
    "# missing_tweet = api.statuses_lookup([847652506372984835],trim_user=True)\n",
    "# missing_tweet\n",
    "hey = '         '\n",
    "hey.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining tweets \n",
    "status_ids_fetched = []\n",
    "outputs_context = []\n",
    "for output in outputs:\n",
    "    status_id = str(output['in_reply_to_status_id'])\n",
    "    if status_id in replies:\n",
    "        output['reply_to_tweet_text'] = output['text'] + replies[status_id]\n",
    "    else:\n",
    "        output['reply_to_tweet_text'] = output['text'] + output['text']\n",
    "    outputs_context.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "\n",
    "def tokenize(outpus):\n",
    "    key = 'reply_to_tweet_text'\n",
    "    outputs_processed = []\n",
    "    for output in outputs:\n",
    "        text = output[key]\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        tokens = text.split(' ')\n",
    "        output['tokens'] = tokens\n",
    "        outputs_processed.append(output)\n",
    "    return outputs_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_processed = tokenize(outputs_context)\n",
    "outputs_processed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_EMBED_DIM = 300\n",
    "TWITTER_EMBED_DIM = 400\n",
    "TWEET_SENTENCE_SIZE = 17*2 # 16 is average tweet token length\n",
    "TWEET_WORD_SIZE = 20 # selected by histogram of tweet counts\n",
    "FASTTEXT_EMBED_DIM = 300\n",
    "EMBED_DIM = 200\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "def generate_random_embedding(embed_dim):\n",
    "    return np.random.normal(scale=0.6, size=(embed_dim,))\n",
    "\n",
    "def process_tweet(tweet, embed_dim, word_vectors):\n",
    "    embedded_tweet = []\n",
    "\n",
    "    # trim if too large\n",
    "    if len(tweet) >= TWEET_SENTENCE_SIZE:\n",
    "        tweet = tweet[:TWEET_SENTENCE_SIZE]\n",
    "\n",
    "    # convert all into word embeddings\n",
    "    for word in tweet:\n",
    "        embedding = generate_random_embedding(embed_dim) if word not in word_vectors else word_vectors[word]\n",
    "        embedded_tweet.append(embedding)\n",
    "\n",
    "    # pad if too short\n",
    "    if len(tweet) < TWEET_SENTENCE_SIZE:\n",
    "        diff = TWEET_SENTENCE_SIZE - len(tweet)\n",
    "        embedded_tweet += [generate_random_embedding(embed_dim) for _ in range(diff)]\n",
    "    return embedded_tweet\n",
    "\n",
    "# embeds tokens! \n",
    "def fetch_word_embeddings(outputs, word_vectors, embed_dim, experiment_flag):\n",
    "    outputs_embed = []\n",
    "    for i, output in enumerate(outputs):\n",
    "\n",
    "        # process first tweet\n",
    "        embedded_tweet = process_tweet(output['tokens'], embed_dim, word_vectors)\n",
    "\n",
    "        if experiment_flag == 2:\n",
    "            #proceses second tweet\n",
    "            if output['context_tweet'] is None:\n",
    "                for i in range(TWEET_SENTENCE_SIZE):\n",
    "                    blank_embedding = np.zeros(embed_dim,)\n",
    "                    embedded_tweet.append(blank_embedding)\n",
    "            else:\n",
    "                context_embedding = process_tweet(output['context_tokens'], embed_dim, word_vectors)\n",
    "                for i in range(TWEET_SENTENCE_SIZE):\n",
    "                    embedded_tweet.append(context_embedding[i])\n",
    "            assert len(embedded_tweet) == TWEET_SENTENCE_SIZE*2\n",
    "\n",
    "        # add real numbers \n",
    "        if experiment_flag == 3:\n",
    "            for i, embed in enumerate(embedded_tweet):\n",
    "                print(np.array(embed).shape)\n",
    "                embedded_tweet[i] = np.concatenate((embed, [output['retweet_count'], output['favorite_count']]))\n",
    "                \n",
    "        output['embedding'] = embedded_tweet\n",
    "        print(np.array(embedded_tweet).shape)\n",
    "        break\n",
    "        outputs_embed.append(output)\n",
    "    return outputs_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "embed_dim = 400\n",
    "filename = os.path.join(ROOT_DIR, 'data/word2vec_twitter_model/word2vec_twitter_model.bin')\n",
    "word_vectors = KeyedVectors.load_word2vec_format(filename, binary=True, unicode_errors='ignore')\n",
    "print(\"Total time {} min\".format((time.time() - start) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "x_train_embed = fetch_word_embeddings(x_train, word_vectors, embed_dim, 3)\n",
    "# x_valid_embed = fetch_word_embeddings(x_valid, word_vectors, embed_dim)\n",
    "# x_test_embed = fetch_word_embeddings(x_test, word_vectors, embed_dim)\n",
    "# print(\"Total time {} min\".format((time.time() - start) / 60))\n",
    "# print(len(x_train_embed), len(x_valid_embed), len(x_test_embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def convert_to_feature_embeddings(x_embed, experiment_flag, key='embedding'):\n",
    "    \"\"\"\n",
    "    :param x_embed: dictionary with all params of processed tweets\n",
    "    :param key: what params should be kept\n",
    "    :param experiment_flag: which type of experiment\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print(\"Experiment flag {}\".format(experiment_flag))\n",
    "    if key == 'tokens': #  for tdidf\n",
    "        if experiment_flag == 1:\n",
    "            return [x['tweet'] for x in x_embed]\n",
    "        else:\n",
    "            output = []\n",
    "            for x in x_embed:\n",
    "                if x['context_tweet']:\n",
    "                    output.append(x['tweet'] + '\\n' + x['context_tweet'])\n",
    "                else:\n",
    "                    output.append(x['tweet'] + '\\n' + ' '.join([' '] * len(x['tweet'])))\n",
    "            return output\n",
    "    return [x[key] for x in x_embed]\n",
    "\n",
    "vectorizer = TfidfVectorizer(use_idf=True, max_features=10000)\n",
    "x_train_embed = vectorizer.fit_transform(convert_to_feature_embeddings(x_train,\n",
    "                                                                       key='tokens',\n",
    "                                                                       experiment_flag=3\n",
    "                                                                       )).todense()\n",
    "x_valid_embed = vectorizer.transform(convert_to_feature_embeddings(x_valid,\n",
    "                                                                   key='tokens',\n",
    "                                                                   experiment_flag=3\n",
    "                                                                   )).todense()\n",
    "x_test_embed = vectorizer.transform(convert_to_feature_embeddings(x_test,\n",
    "                                                                  key='tokens',\n",
    "                                                                          experiment_flag=3\n",
    "                                                                          )).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tdidf_experiment_3_embeddings(data_list, embeddings):\n",
    "    processed_embeddings = []\n",
    "    for i, embed in enumerate(embeddings):\n",
    "        embed = np.transpose(np.array(embed))\n",
    "        embed = embed.reshape(-1)\n",
    "        output = data_list[i]\n",
    "        processed_embeddings.append(np.concatenate((embed, [output['retweet_count'], output['favorite_count']])))\n",
    "    return processed_embeddings\n",
    "\n",
    "x_train_embed = generate_tdidf_experiment_3_embeddings(x_train, x_train_embed)\n",
    "print(np.array(x_train_embed).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_embed[0].keys() # list of dictionaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0] #int "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17 n-gram \n",
    "# retweet count \n",
    "# in reply to status id \n",
    "# favorite count \n",
    "\n",
    "# 34 n-gram \n",
    "def convert_to_feature_embeddings(x_embed):\n",
    "    return [x['word_embeddings'] for x in x_embed]\n",
    " \n",
    "data = {}\n",
    "print(x_train_embed[0].keys())\n",
    "data['x_train'] = convert_to_feature_embeddings(x_train_embed)\n",
    "data['y_train'] = y_train\n",
    "data['x_valid'] = convert_to_feature_embeddings(x_valid_embed)\n",
    "data['y_valid'] = y_valid\n",
    "data['x_test'] = convert_to_feature_embeddings(x_test_embed)\n",
    "data['y_test'] = y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "def wrap_data(batch_size, seed, x_train, y_train, x_valid, y_valid, x_test, y_test):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    transform=None\n",
    "    \n",
    "    train_set = DataProvider(inputs=x_train, targets=y_train, seed=seed, transform=transform)\n",
    "    train_data_local = torch.utils.data.DataLoader(train_set,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   num_workers=2,\n",
    "                                                   sampler=ImbalancedDatasetSampler(train_set),\n",
    "                                                   )\n",
    "\n",
    "    valid_set = DataProvider(inputs=x_valid, targets=y_valid, seed=seed, transform=transform)\n",
    "    valid_data_local = torch.utils.data.DataLoader(valid_set,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   num_workers=2,\n",
    "                                                   shuffle=False,\n",
    "                                                  )\n",
    "\n",
    "    test_set = DataProvider(inputs=x_test, targets=y_test, seed=seed, transform=transform)\n",
    "    test_data_local = torch.utils.data.DataLoader(test_set,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  num_workers=2,\n",
    "                                                  shuffle=False,\n",
    "                                                 )\n",
    "    return train_data_local, valid_data_local, test_data_local\n",
    "\n",
    "def fetch_model(model, embedding_level, input_shape_local, dropout):\n",
    "    if model == 'MLP':\n",
    "        return multi_layer_perceptron(input_shape_local)\n",
    "    if model == 'CNN':\n",
    "        if embedding_level == 'word':\n",
    "            return word_cnn(input_shape_local, dropout)\n",
    "        elif embedding_level == 'character':\n",
    "            return character_cnn(input_shape_local)\n",
    "    if model == 'DENSENET':\n",
    "        return densenet()\n",
    "    else:\n",
    "        raise ValueError(\"Model key not found {}\".format(embedding_level))\n",
    "\n",
    "\n",
    "def fetch_model_parameters(input_shape_local):\n",
    "    model_local = fetch_model(model='CNN',\n",
    "                            embedding_level='word',\n",
    "                            input_shape_local=input_shape_local,\n",
    "                            dropout=0.5)\n",
    "    criterion_local = torch.nn.CrossEntropyLoss()\n",
    "    optimizer_local = torch.optim.Adam(model_local.parameters(), weight_decay=1e-4)\n",
    "    scheduler_local = optim.lr_scheduler.CosineAnnealingLR(optimizer_local, T_max=100, eta_min=0.0001)\n",
    "    return model_local, criterion_local, optimizer_local, scheduler_local\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = wrap_data(2048, 28, **data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in train_data:\n",
    "    input_shape = x.shape\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = tuple(input_shape)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train_iter(model, device, optimizer, criterion, x, y, stats, experiment_key='train'):\n",
    "    \"\"\"\n",
    "    Receives the inputs and targets for the model and runs a training iteration. Returns loss and accuracy metrics.\n",
    "    :param x: The inputs to the model. A numpy array of shape batch_size, channels, height, width\n",
    "    :param y: The targets for the model. A numpy array of shape batch_size, num_classes\n",
    "    :return: the loss and accuracy for this batch\n",
    "    \"\"\"\n",
    "    # sets model to training mode\n",
    "    # (in case batch normalization or other methods have different procedures for training and evaluation)\n",
    "    model.train()\n",
    "    x = x.float()\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    optimizer.zero_grad()  # set all weight grads from previous training iters to 0\n",
    "    out = model.forward(x)  # forward the data in the model\n",
    "    # loss = F.cross_entropy(input=out, target=y)  # compute loss\n",
    "    loss = criterion(out, y)\n",
    "    loss.backward()  # backpropagate to compute gradients for current iter loss\n",
    "\n",
    "    optimizer.step()  # update network parameters\n",
    "    _, predicted = torch.max(out.data, 1)  # get argmax of predictions\n",
    "    accuracy = np.mean(list(predicted.eq(y.data).cpu()))  # compute accuracy\n",
    "    stats['{}_acc'.format(experiment_key)].append(accuracy)\n",
    "    stats['{}_loss'.format(experiment_key)].append(loss.data.detach().cpu().numpy())\n",
    "\n",
    "def run_evaluation_iter(model, device, optimizer, criterion, x, y, stats, experiment_key='valid'):\n",
    "    \"\"\"\n",
    "    Receives the inputs and targets for the model and runs an evaluation iterations. Returns loss and accuracy metrics.\n",
    "    :param x: The inputs to the model. A numpy array of shape batch_size, channels, height, width\n",
    "    :param y: The targets for the model. A numpy array of shape batch_size, num_classes\n",
    "    :return: the loss and accuracy for this batch\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        model.eval()  # sets the system to validation mode\n",
    "        x = x.float()\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        out = model.forward(x)  # forward the data in the model\n",
    "        loss = criterion(out, y)\n",
    "        \n",
    "        # loss = F.cross_entropy(out, y)  # compute loss\n",
    "        _, predicted = torch.max(out.data, 1)  # get argmax of predictions\n",
    "        \n",
    "        accuracy = np.mean(list(predicted.eq(y.data).cpu()))\n",
    "        stats['{}_acc'.format(experiment_key)].append(accuracy)  # compute accuracy\n",
    "        stats['{}_loss'.format(experiment_key)].append(loss.data.detach().cpu().numpy())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_save_dir, model_save_name, model_idx):\n",
    "    \"\"\"\n",
    "    Save the network parameter state and current best val epoch idx and best val accuracy.\n",
    "    :param model_save_name: Name to use to save model without the epoch index\n",
    "    :param model_idx: The index to save the model with.\n",
    "    :param best_validation_model_idx: The index of the best validation model to be stored for future use.\n",
    "    :param best_validation_model_acc: The best validation accuracy to be stored for use at test time.\n",
    "    :param model_save_dir: The directory to store the state at.\n",
    "    :param state: The dictionary containing the system state.\n",
    "\n",
    "    \"\"\"\n",
    "    # Save state each epoch\n",
    "    path = os.path.join(model_save_dir, \"{}_{}\".format(model_save_name, str(model_idx)))\n",
    "    torch.save(model.state_dict(), f=path)\n",
    "    \n",
    "\n",
    "def load_model(model, model_save_dir, model_save_name, model_idx):\n",
    "    \"\"\"\n",
    "    Load the network parameter state and the best val model idx and best val acc to be compared with the future val accuracies, in order to choose the best val model\n",
    "    :param model_save_dir: The directory to store the state at.\n",
    "    :param model_save_name: Name to use to save model without the epoch index\n",
    "    :param model_idx: The index to save the model with.\n",
    "    \"\"\"\n",
    "    path = os.path.join(model_save_dir, \"{}_{}\".format(model_save_name, str(model_idx)))\n",
    "    checkpoint = torch.load(f=path)\n",
    "    # freeze parameters\n",
    "    model.load_state_dict(checkpoint)\n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict, defaultdict\n",
    "import tqdm\n",
    "\n",
    "model, criterion, optimizer, _ = fetch_model_parameters(input_shape)\n",
    "device = torch.device('cpu')\n",
    "train_stats = OrderedDict()\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    epoch_stats = defaultdict(list)\n",
    "    with tqdm.tqdm(total=len(train_data)) as pbar_train:  # create a progress bar for training\n",
    "        for idx, (x, y) in enumerate(train_data):  # get data batches\n",
    "            run_train_iter(model, device, optimizer, criterion, x=x, y=y, stats=epoch_stats)  # take a training iter step\n",
    "            pbar_train.update(1)\n",
    "            pbar_train.set_description(\"loss: {:.4f}, accuracy: {:.4f}\".format(epoch_stats['train_loss'][-1],\n",
    "                                                                               epoch_stats['train_acc'][-1]))\n",
    "\n",
    "    with tqdm.tqdm(total=len(valid_data)) as pbar_val:  # create a progress bar for validation\n",
    "        for x, y in valid_data:  # get data batches\n",
    "            run_evaluation_iter(model, device, optimizer, criterion, x=x, y=y, stats=epoch_stats)  # run a validation iter\n",
    "            pbar_val.update(1)  # add 1 step to the progress bar\n",
    "            pbar_val.set_description(\"loss: {:.4f}, accuracy: {:.4f}\".format(epoch_stats['valid_loss'][-1],\n",
    "                                                                             epoch_stats['valid_acc'][-1]))\n",
    "     \n",
    "    \n",
    "    \n",
    "    save_model(model, '', 'testing', epoch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model(model, '', 'testing', 1)\n",
    "\n",
    "#evaluate test here\n",
    "with tqdm.tqdm(total=len(test_data)) as pbar_test:  # create a progress bar for validation\n",
    "    for x, y in test_data:  # get data batches\n",
    "        run_evaluation_iter(model, device, optimizer, criterion, x=x, y=y, stats=epoch_stats, experiment_key=\"test_experiment\")  # run a validation iter\n",
    "        pbar_test.update(1)  # add 1 step to the progress bar\n",
    "        pbar_test.set_description(\"loss: {:.4f}, accuracy: {:.4f}\".format(epoch_stats['test_experiment_loss'][-1],\n",
    "                                                                  epoch_stats['test_experiment_acc'][-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "for i in range(17):\n",
    "    arr.append(np.zeros(200,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(arr).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_tweet = np.array(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_tweet += np.array(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_tweet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_tweet = np.array(arr) + np.array(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_tweet = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(17):\n",
    "    blank_embedding = np.zeros(200,)\n",
    "    embedded_tweet.append(blank_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(embedded_tweet).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_shape = (2048, 24) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.zeros(_shape)\n",
    "y = np.zeros(_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate((x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.Tensor(x)\n",
    "y = torch.Tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = torch.cat((x, y), 1)\n",
    "sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
